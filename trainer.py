# trainer.py
import os
import time
import sys
from datetime import datetime

from brain import Brain
from brain_es import EvolutionStrategy
from sensors import read_sensors, compute_reward
from move import forward, backward, left, right, stop


# ---------------------------
# ì„¸ëŒ€ ìˆ˜ ì¸ì ì²˜ë¦¬
# ---------------------------
if len(sys.argv) >= 2:
    try:
        TARGET_GENERATIONS = int(sys.argv[1])
    except:
        TARGET_GENERATIONS = 10
else:
    TARGET_GENERATIONS = 10

print(f"[INFO] Training for {TARGET_GENERATIONS} generations.")


# ---------------------------
# ë¡œê·¸ ë””ë ‰í† ë¦¬ + íŒŒì¼ëª… ë‚ ì§œ í¬í•¨
# ---------------------------
LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
LOG_FILE = f"{LOG_DIR}/es_train_{timestamp}.log"

def log(message):
    t = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a") as f:
        f.write(f"[{t}] {message}\n")


# ---------------------------
# ëª¨ë¸ ì´ˆê¸°í™”
# ---------------------------
brain = Brain()
weights = brain.get_weights()
es = EvolutionStrategy(weights, population=5)

generation = 0


# ---------------------------
# ë©”ì¸ í•™ìŠµ ë£¨í”„
# ---------------------------
while generation < TARGET_GENERATIONS:

    generation += 1
    print(f"\n========== Generation {generation}/{TARGET_GENERATIONS} ==========\n")
    log(f"----- Generation {generation} -----")

    noises = es.ask()
    rewards = []

    for idx, noise in enumerate(noises):
        print(f"[Gen {generation}] Testing individual {idx+1}/{len(noises)}")

        # ì‹ ê·œ ê°€ì¤‘ì¹˜ ì ìš©
        new_weights = [w + n * es.sigma for w, n in zip(weights, noise)]
        brain.set_weights(new_weights)

        s = read_sensors()
        action = brain.act(s)

        # í–‰ë™ ì‹¤í–‰
        if action == "RUN_AWAY":
            backward(speed=60)
        elif action == "APPROACH":
            forward(speed=50)
        elif action == "TURN_LEFT":
            left(speed=40)
        elif action == "TURN_RIGHT":
            right(speed=40)

        time.sleep(0.15)

        r = compute_reward(s)
        rewards.append(r)

        # ìƒì„¸ ì¶œë ¥
        print(f"  Sensor: {s}")
        print(f"  Action: {action}")
        print(f"  Reward: {r:.3f}")

        # ë¡œê·¸ ì €ì¥
        log(f"Gen{generation} | Ind{idx+1} | Sensor={s} | Action={action} | Reward={r:.3f}")

    stop()
    time.sleep(0.1)

    weights = es.update(noises, rewards)
    brain.set_weights(weights)

    avg_r = sum(rewards) / len(rewards)
    print(f"\n[Generation {generation}] Average Reward: {avg_r:.3f}")
    log(f"Generation {generation} Average Reward: {avg_r:.3f}")

print("\nğŸ‰ Training complete!")
print(f"Log saved: {LOG_FILE}")
